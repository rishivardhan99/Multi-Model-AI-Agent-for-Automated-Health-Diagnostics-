
# ğŸ§  Multi-Model AI Agent for Automated Health Diagnostics

A **production-grade, research-oriented multi-model AI system** for automated blood report analysis.  
The system integrates **OCR, deterministic clinical reasoning, knowledge-graphâ€“based inference, and LLM-powered narrative synthesis** into a single, auditable pipeline. It includes a new evaluation module (`llmevals/`) for honest model evaluation and a lightweight chatbot assistant (`chatbot/`) for interactive exploration.



> âš ï¸ **Medical Disclaimer**  
> This platform is strictly an **assistive decision-support system** intended for educational and research use.  
> It does **not** provide diagnoses, prescriptions, or treatment recommendations. All outputs must be reviewed by a qualified medical professional.

---


## âœ¨ Key Capabilities

- ğŸ“„ Accepts **PDF / Image / JSON** blood reports  
- ğŸ” OCR with robust parameter extraction  
- ğŸ§ª Parameter-level clinical interpretation (Normal / Low / High)  
- ğŸ§  Deterministic pattern detection & probabilistic risk inference (Model-2)  
- ğŸ§© Knowledge-graphâ€“based causal reasoning  
- âœï¸ LLM-based **explainable medical narratives** (Model-3) with safety guardrails  
- âœ… Evaluation framework (`llmevals/`) with explicit abstention and validation logic  
- ğŸ–¥ï¸ Interactive **Streamlit UI** and small **chatbot** for exploration  
- ğŸ³ **Dockerized** for reproducibility

---

## ğŸ§­ System Architecture

```
Input (PDF / Image / JSON)
        â†“
Extractor (OCR + Parsing)
        â†“
Model-1 (Clinical Parameter Normalization)
        â†“
Model-2 (Pattern, Risk & Causal Reasoning)
        â†“
Model-3 (LLM Narrative Synthesis)
        â†“
LLMEVALS (Evaluation, Validation, Aggregation)  â† optional offline step
        â†“
Final Report + Auditable Artifacts + Streamlit UI / Chatbot
```

> The system is intentionally **layered**:
> - Clinical facts are determined deterministically (Model-1 & Model-2)
> - LLMs are used only for **synthesis and communication** (Model-3)
> - Evaluation & safety (llmevals) enforces auditability and honest metrics

---

## ğŸ§© Component Breakdown

### 1ï¸âƒ£ Extractor â€” OCR & Structuring
**Location:** `extractor/`

**Responsibilities**
- OCR for scanned PDFs and images  
- Text normalization and cleanup  
- Parameter detection with plausibility checks  
- Conversion into structured CSV / JSON

**Outputs**
```
outputs/structured_per_report/<file>.structured.csv
outputs/model1_per_report/<file>.model1_final.csv
```

---

### 2ï¸âƒ£ Model-1 â€” Clinical Parameter Normalization
**Purpose:** Deterministic interpretation of extracted lab values.

**Key behavior**
- Compares values against reference ranges  
- Assigns status labels: Normal / Low / High  
- Produces parameter-level notes for downstream reasoning

**Output**
```
outputs/model1_per_report/<file>.model1_final.csv
```

> Model-1 contains **no probabilistic logic** â€” it establishes factual ground truth.

---

### 3ï¸âƒ£ Model-2 â€” Pattern, Risk & Causal Reasoning
**Location:** `model2/`

**Design**
- Fully **deterministic and auditable** (no LLM dependency)
- Pattern detection (e.g., anemia), derived metrics, knowledge-graph causal links
- Confidence scoring based on evidence completeness

**Key files**
```
model2/
â”œâ”€â”€ model2_runner.py
â”œâ”€â”€ serializer.py
â”œâ”€â”€ verifier.py
â””â”€â”€ pipeline/
    â”œâ”€â”€ loader.py
    â”œâ”€â”€ pattern_engine.py
    â”œâ”€â”€ probable_causes.py
    â”œâ”€â”€ knowledge_graph.py
    â”œâ”€â”€ risk_engine.py
    â””â”€â”€ confidence.py
```

**Outputs**
```
outputs/model2_outputs/<file>.model2.json
outputs/model2_outputs/<file>.model2.txt
```

> Model-2 performs **reasoning**, not narration.

---

### 4ï¸âƒ£ Model-3 â€” LLM Narrative Synthesis
**Location:** `model3/`

**Design philosophy**
- **LLM used only for synthesis/explanation** â€” not for establishing clinical facts
- Strict prompt contract: do not invent facts, prefer cautious language, return only JSON matching the schema
- Deterministic fallback when LLM fails

**Key files**
```
model3/
â”œâ”€â”€ model3_runner.py
â”œâ”€â”€ prompts.py
â”œâ”€â”€ schema_model3.json
â”œâ”€â”€ guardrails.py
â””â”€â”€ gemini_client.py
```

**Important prompt rules added (Model-3):**
- Rule 11: *Do NOT interpret numeric values as abnormal unless Model-2 flags a pattern.*
- Rule 13 (new): *If a complete group of related measurements is provided (e.g., lipid profile, CBC) and no Model-2 abnormal pattern is flagged, explicitly state that no significant abnormal pattern is identified for that group.*  
  â€” prevents omission ambiguity.
- Rule 14 (new): *If Model-2 relies on a single signal, Model-3 may note limited evidence using cautious language (e.g., "may benefit from clinical correlation") but must NOT negate or reclassify Model-2.*

**Outputs**
```
outputs/model3/<file>.model3.json
outputs/model3/<file>.model3.txt
outputs/model3/<file>.model3.prompt.txt
```

---

### 5ï¸âƒ£ LLMEVALS â€” Evaluation & Validation
**Location:** `llmevals/llmevals_pkg`

**Purpose:** Honest, auditable automated evaluation comparing Model-3 output to Model-2 (ground truth) with explicit abstention handling and validation.

**Core design decisions**
- **Do not count technical outages** (LLM or API failures) against model quality â€” those are *technical abstentions* and excluded from the denominator.
- **Safety-first abstentions** (clinical abstentions) are treated as **correct system behavior** (system passes) â€” e.g., when Model-3 hallucinates or omits critical high-confidence Model-2 signals.
- Post-hoc validation applies penalties for `hallucination` and `missing` issues and computes a validated score.
- Aggregation excludes technical abstentions from denominator; clinical abstentions are included and count as passes.

**Validation rules (implemented)**
- Penalty weights for issues (configurable):
  - hallucination high/medium/low â†’ 20 / 10 / 5 points
  - missing high/medium/low â†’ 10 / 4 / 2 points
- `critical_missing_count`: counts only **high-severity** missing issues (these represent high-confidence Model-2 signals omitted by Model-3).
- **Abstention types**:
  - `technical` â€” infra/parse errors (excluded)
  - `clinical` â€” high-severity hallucination or â‰¥2 critical missing signals (system abstains for safety; counts as pass)
- Soft pass threshold (env): `SOFT_PASS_THRESHOLD` (default 60.0)
- System pass threshold (env): `SYSTEM_PASS_THRESHOLD` (default 90.0 is a different configured threshold and may be used where desired)

**Key files**
```
llmevals/
â”œâ”€â”€ llmevals_pkg/
â”‚   â”œâ”€â”€ client.py             # LLM client (groq/openai compat)
â”‚   â”œâ”€â”€ evaluator.py          # main evaluation driver
â”‚   â”œâ”€â”€ prompt_templates.py   # pairwise & merge prompt templates
â”‚   â”œâ”€â”€ report.py             # final report / md export
â”‚   â””â”€â”€ validation.py        # validation & abstention rules
â””â”€â”€ run_eval.py               # CLI entrypoint
```

**How the aggregate is computed (plain)**
- Denominator = `system_total_evaluated` = number of non-technical evaluations (technical abstentions excluded)
- Numerator = `validated_system_pass_count` = count of items where `system_pass == True`
- `validated_system_accuracy` = Numerator / Denominator Ã— 100
- `abstention_count` counts **clinical abstentions** (safety behavior), `abstention_rate` = abstentions / Denominator

**Outputs**
```
outputs/llmevals/final_report.json
outputs/llmevals/final_report.md
outputs/llmevals/individual_evals/*.eval.json
```

---

## ğŸ—‚ Chatbot â€” lightweight interactive assistant
**Location:** `chatbot/`

**Purpose:** Small conversational tool to:
- query a processed report
- view Model-2 patterns and Model-3 narrative
- test prompt variations quickly
- simulate human-in-the-loop escalation workflows

**Note:** Chatbot is intentionally *exploratory* and not part of the production evaluation pipeline. Use for developer testing and demos.

---

## ğŸ–¥ï¸ Streamlit Application
**Entry point:** `app.py`

**Features**
- File upload (PDF / Image / JSON)
- Patient & context input
- Step-by-step pipeline execution
- Model-2 reasoning visualization
- Model-3 narrative output
- Artifact downloads and debug/audit views

Run locally:
```bash
streamlit run app.py
```

---

## ğŸ³ Docker Setup

**UI Docker (Streamlit)**

Build:
```bash
docker build -f Dockerfile.ui -t medicube-ai .
```

Run:
```bash
docker run -p 8501:8501 --env-file .env medicube-ai
# open http://localhost:8501
```

**LLMEVALS Docker** (example for evaluation runs)
You may prefer a separate image for batch evaluation.

Build:
```bash
docker build -f Dockerfile.llmevals -t medicube-llmevals .
```

Run (PowerShell-friendly example):
```powershell
docker run --rm `
  -e CLIENT_TYPE=groq `
  -e MODEL_NAME=llama-3.1-8b-instant `
  -e MAX_TOKENS=1024 `
  -e MAX_CHARS_PER_ITEM=3000 `
  -e MAX_INPUT_CHARS_PER_REQUEST=12000 `
  -e EVAL_MODE=pairwise `
  --env-file .env `
  -v "${PWD}\inputs:/app/inputs" `
  -v "${PWD}\outputs:/app/outputs" `
  medicube-llmevals
```

> **Security reminder:** never commit `.env` or API keys to Git. Add `.env` to `.gitignore`.

---

## ğŸ”§ Environment variables (key ones used by llmevals & runners)

- `CLIENT_TYPE` â€” `groq_http` or `openai_compat`
- `GROQ_API_KEY`, `GROQ_API_URL`
- `OPENAI_API_KEY`, `OPENAI_API_BASE`
- `MODEL_NAME` â€” model identifier
- `MAX_TOKENS` â€” tokens allocated to evaluator LLM calls
- `MAX_CHARS_PER_ITEM` â€” compact per-item text cap (default 3000)
- `MAX_INPUT_CHARS_PER_REQUEST` â€” input limit per merge request (default ~12000)
- `MERGE_BATCH_SIZE` â€” merge mode hint
- `SOFT_PASS_THRESHOLD` â€” soft threshold for `system_pass` (default 60.0)
- `SYSTEM_PASS_THRESHOLD` â€” optional system pass threshold (default 90.0)
- `EVAL_MODE` â€” `pairwise` or `merge`

---

## âœ… Best practices & guidelines

- **Do not** allow Model-3 to *contradict* Model-2. Model-3 may express **cautious uncertainty** but must not reclassify Model-2 patterns. (See Model-3 rules 14 and 11/13.)
- **Exclude technical failures** from accuracy calculations â€” these are infra issues.
- **Count clinical abstentions as correct system behavior** â€” forced abstention for safety is desirable.
- **Keep prompts concise** and include explicit instructions to return valid JSON only.
- **Audit everything**: store inputs, prompts, raw LLM outputs, and parsed JSON in `outputs/` for reproducibility.

---

## ğŸ“„ Example llmevals output (short)

`final_report.json` provides:
- `mode` (pairwise / merge)
- `individuals` (array of per-file eval + validation)
- `aggregate`:
  - `llm_raw_count`, `llm_raw_mean`, ...
  - `system_total_evaluated`, `validated_system_pass_count`, `validated_system_accuracy`
  - `abstention_count`, `abstention_rate`
  - `high_severity_issues` list

Per-file `*.eval.json` contains:
```json
{
  "filename": "clean_00012.model1_final",
  "model2_path": "...",
  "model3_path": "...",
  "eval": { ... },
  "validation": {
    "raw_score": 80.0,
    "penalty": 34.0,
    "validated_score": 46.0,
    "is_abstain": true,
    "abstention_type": "clinical",
    "system_pass": true,
    "reasons": ["clinical_abstention_due_to_high_risk_output"]
  }
}
```

---

## ğŸ“ Versioning & release notes (recommended)
When you push changes:
- Tag release versions (e.g., `v0.4-llmevals`)
- Add a short `CHANGELOG.md` noting:
  - `llmevals` added with abstention/validation rules
  - `chatbot` added for interactive testing
  - Model-3 prompt updates (Rule 13 & Rule 14)
  - Any breaking changes to output schemas

---

## ğŸ” Safety & compliance notes

- No medication dosing or prescriptive instructions are generated.
- Model-3 must always use cautious phrasing; hallucinations and unsupported claims are penalized.
- All sensitive keys **must** remain out of git.
- For any deployment beyond research, consult clinical advisors and comply with local health regulations.

---

## ğŸ“„ License

For academic, educational, and research use only.

---

## ğŸ Quick developer checklist before you push changes

1. `git status` â†’ verify files to commit
2. Make sure `.env` is in `.gitignore`
3. `git add .` â†’ `git commit -m "..."` â†’ `git push origin main`
4. Tag release: `git tag -a v0.4 -m "Add llmevals & chatbot; Model-3 prompt updates"` â†’ `git push --tags`
5. Run a quick llmevals dry run on a small sample to validate aggregation

---
